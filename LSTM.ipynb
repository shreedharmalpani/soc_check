{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreedharmalpani/soc_check/blob/main/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F66Wy9TAH9v_"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "PY2Cz78qIpIW",
        "outputId": "0357d56a-a46d-44b3-df03-8a2a1e0ca887"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IMDB Dataset.csv')\n",
        "print(df.columns)\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['review', 'sentiment'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  negative\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t43U2Vs0uIL9"
      },
      "source": [
        "def splitDF(r):\n",
        "  dataLen = len(df)\n",
        "  temp = ['test']*int((1-r)*dataLen) + ['train']*int((r)*dataLen)\n",
        "  random.shuffle(temp)\n",
        "  df['split'] = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMGqe_JbIr_T",
        "outputId": "e704e86c-c527-48b3-f198-9cad33b16be3"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjUZsNSsbON3"
      },
      "source": [
        "df['review'] = df['review'].apply(lambda x:x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef1JjRa_Iw_w"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "SlYUGNXDI1fm",
        "outputId": "a4c6e7cc-d24f-445f-b4fb-4ac5490ee802"
      },
      "source": [
        "df['review'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"one reviewers mentioned watching 1 oz episode hooked. right, exactly happened me.<br /><br />the first thing struck oz brutality unflinching scenes violence, set right word go. trust me, show faint hearted timid. show pulls punches regards drugs, sex violence. hardcore, classic use word.<br /><br />it called oz nickname given oswald maximum security state penitentary. focuses mainly emerald city, experimental section prison cells glass fronts face inwards, privacy high agenda. em city home many..aryans, muslims, gangstas, latinos, christians, italians, irish more....so scuffles, death stares, dodgy dealings shady agreements never far away.<br /><br />i would say main appeal show due fact goes shows dare. forget pretty pictures painted mainstream audiences, forget charm, forget romance...oz mess around. first episode ever saw struck nasty surreal, say ready it, watched more, developed taste oz, got accustomed high levels graphic violence. violence, injustice (crooked guards who'll sold nickel, inmates who'll kill order get away it, well mannered, middle class inmates turned prison bitches due lack street skills prison experience) watching oz, may become comfortable uncomfortable viewing....thats get touch darker side.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBDNtK9mI5t4"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', ',','#','$','@','%','^','&','*'))\n",
        "    return final\n",
        "\n",
        "df['review'] = df['review'].apply(remove_punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv6U7EPEJE4T"
      },
      "source": [
        "def remove_tags(text):\n",
        "    final = \"\"\n",
        "    stt = True\n",
        "    for char in text:\n",
        "        if char == '<':\n",
        "            stt = False\n",
        "        if(stt):\n",
        "            final = final + char\n",
        "        if char == '>':\n",
        "            stt = True\n",
        "            final = final + ' '\n",
        "    return final\n",
        "    \n",
        "df['review'] = df['review'].apply(remove_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "QeaQaRnGJMtN",
        "outputId": "e66955fb-d9c0-4f7a-b53d-983abfc1dc5d"
      },
      "source": [
        "df['review'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"one reviewers mentioned watching 1 oz episode hooked right exactly happened me  the first thing struck oz brutality unflinching scenes violence set right word go trust me show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word  it called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far away  i would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz mess around first episode ever saw struck nasty surreal say ready it watched more developed taste oz got accustomed high levels graphic violence violence injustice (crooked guards who'll sold nickel inmates who'll kill order get away it well mannered middle class inmates turned prison bitches due lack street skills prison experience) watching oz may become comfortable uncomfortable viewingthats get touch darker side\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2qHZWigJSX9",
        "outputId": "aa4938a7-efff-4c0e-d1b6-96c79300b60e"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7vA7hWgJWCA"
      },
      "source": [
        "df['review'] = df['review'].apply(nltk.word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biJRHSq0JaCq"
      },
      "source": [
        "def stem_tokens(tokens):\n",
        "    final = [nltk.stem.PorterStemmer().stem(word) for word in tokens]\n",
        "    return final\n",
        "    \n",
        "df['review'] = df['review'].apply(stem_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atxLAQQ-CKPB"
      },
      "source": [
        "df['len_review'] = df['review'].apply(lambda x:len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbL85Zq6Jebv"
      },
      "source": [
        "df['sentiment'] = [1*(sent=='positive') for sent in df['sentiment']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "nNVRUsavJnS9",
        "outputId": "02a4e3ed-c670-44c3-f214-97ade2ab9800"
      },
      "source": [
        "df.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>len_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[one, review, mention, watch, 1, oz, episod, h...</td>\n",
              "      <td>1</td>\n",
              "      <td>175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[wonder, littl, product, the, film, techniqu, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
              "      <td>1</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[basic, there, 's, famili, littl, boy, (, jake...</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment  len_review\n",
              "0  [one, review, mention, watch, 1, oz, episod, h...          1         175\n",
              "1  [wonder, littl, product, the, film, techniqu, ...          1          96\n",
              "2  [thought, wonder, way, spend, time, hot, summe...          1          95\n",
              "3  [basic, there, 's, famili, littl, boy, (, jake...          0          74"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHmIuhdfJrBc"
      },
      "source": [
        "from gensim import corpora\n",
        "\n",
        "review_dict = corpora.Dictionary(df['review'])\n",
        "VOCAB_SIZE = len(review_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3maVA_-c7tLo"
      },
      "source": [
        "review_dict?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXFOb6ryJx7y",
        "outputId": "b3e5b61f-4376-492f-c292-a41d1da78ba6"
      },
      "source": [
        "print(len(review_dict))\n",
        "review_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "132056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.corpora.dictionary.Dictionary at 0x7fc128352a10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxDV3zsMStNi"
      },
      "source": [
        "review_int = []\n",
        "for review in df['review']:\n",
        "  temp = [review_dict.token2id[x] for x in review]\n",
        "  review_int.append(temp)\n",
        "df['review_int'] = review_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "c7wqt154J-C5",
        "outputId": "7d33787b-4f6a-4b36-dac1-80b821c3c7cb"
      },
      "source": [
        "df.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>len_review</th>\n",
              "      <th>review_int</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[one, review, mention, watch, 1, oz, episod, h...</td>\n",
              "      <td>1</td>\n",
              "      <td>175</td>\n",
              "      <td>[90, 104, 80, 137, 3, 93, 33, 60, 105, 35, 55,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[wonder, littl, product, the, film, techniqu, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>96</td>\n",
              "      <td>[206, 171, 183, 126, 162, 196, 202, 175, 161, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
              "      <td>1</td>\n",
              "      <td>95</td>\n",
              "      <td>[265, 206, 269, 256, 266, 230, 260, 271, 255, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[basic, there, 's, famili, littl, boy, (, jake...</td>\n",
              "      <td>0</td>\n",
              "      <td>74</td>\n",
              "      <td>[281, 314, 144, 291, 171, 283, 1, 295, 2, 315,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                                         review_int\n",
              "0  [one, review, mention, watch, 1, oz, episod, h...  ...  [90, 104, 80, 137, 3, 93, 33, 60, 105, 35, 55,...\n",
              "1  [wonder, littl, product, the, film, techniqu, ...  ...  [206, 171, 183, 126, 162, 196, 202, 175, 161, ...\n",
              "2  [thought, wonder, way, spend, time, hot, summe...  ...  [265, 206, 269, 256, 266, 230, 260, 271, 255, ...\n",
              "3  [basic, there, 's, famili, littl, boy, (, jake...  ...  [281, 314, 144, 291, 171, 283, 1, 295, 2, 315,...\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL_A6_CtOnSy"
      },
      "source": [
        "review_int = df['review_int']\n",
        "review_len = df['len_review']\n",
        "review_label = df['sentiment']\n",
        "review_int = [review_int[i] for i,x in enumerate(review_int) if review_len[i]>0]\n",
        "review_label = [review_label[i] for i,x in enumerate(review_label) if review_len[i]>0]\n",
        "review_len = [review_len[i] for i,x in enumerate(review_len) if review_len[i]>0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHxwGLsxQXCZ",
        "outputId": "e3069307-3d1c-4c81-cbf6-9ae3195b8cfc"
      },
      "source": [
        "len(review_int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6mXne9eRKPO"
      },
      "source": [
        "def pad_features(review_int, L):\n",
        "  features = np.zeros((len(review_int),L), dtype = int)\n",
        "  for i,review in enumerate(review_int):\n",
        "    l = len(review)\n",
        "    if l <= L:\n",
        "      patch = [0]*(L-l)\n",
        "      new = patch + review\n",
        "      features[i,:] = np.array(new)\n",
        "    else :\n",
        "      features[i,:] = np.array(review[:L])\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eokRRpqBS7Yv"
      },
      "source": [
        "features = pad_features(review_int, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdos8ybjTLfL",
        "outputId": "0f8e8a0c-eff2-45ad-9fbe-a12c6d1d92fc"
      },
      "source": [
        "print(features[:10,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0 ... 129  24 117]\n",
            " [  0   0   0 ... 197 138 154]\n",
            " [  0   0   0 ...  50 190 227]\n",
            " ...\n",
            " [  0   0   0 ... 404 258 213]\n",
            " [  0   0   0 ... 556 561 562]\n",
            " [  0   0   0 ...  66 165 571]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU1Qzk7nTQgX"
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "train_x = features[0:int(len(features)*split_frac)]\n",
        "train_y = np.array(review_label[0:int(len(features)*split_frac)])\n",
        "\n",
        "test_x = features[len(features)-int(len(features)*split_frac):len(features)-int(len(features)*split_frac/2)]\n",
        "test_y = np.array(review_label[len(features)-int(len(features)*split_frac):len(features)-int(len(features)*split_frac/2)])\n",
        "\n",
        "valid_x = features[len(features)-int(len(features)*split_frac/2):]\n",
        "valid_y = np.array(review_label[len(features)-int(len(features)*split_frac/2):])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaT0yASP6GNq"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVgzEo9jXEMC",
        "outputId": "db4c7fd6-dc92-4463-b035-95b1e3c545fc"
      },
      "source": [
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) \n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size())\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,  2626, 29325,  5189],\n",
            "        [    0,     0,     0,  ...,   633,   581,  8974],\n",
            "        [  522,   404,   299,  ...,  2522,   299,   459],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,  3882,  6065,  4092],\n",
            "        [    0,     0,     0,  ...,   337,   816,   299],\n",
            "        [    0,     0,     0,  ...,    36,    90,    66]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTHG6RNTKE8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c32731-93f6-479c-9df8-f6c0a81d2ef3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available for running: \n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSb3sYFjKQkr"
      },
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "  \n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]\n",
        "        \n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iL3uKFe8pNxu",
        "outputId": "ddad4f82-c464-4392-e8c5-253223f70d6d"
      },
      "source": [
        "\n",
        "vocab_size = len(review_dict)+1 \n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(132057, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWk8YsJEsv0Y",
        "outputId": "61c009dd-f5f1-43b9-b81b-b45947853bfc"
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIPJBJmlszzz"
      },
      "source": [
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8ZC8u7uohwH",
        "outputId": "ab42985c-a34b-4906-9c1a-c4a7826d07ba"
      },
      "source": [
        "\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 \n",
        "\n",
        "\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "\n",
        "for e in range(epochs):\n",
        "    \n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    \n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        \n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        net.zero_grad()\n",
        "\n",
        "      \n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if counter % print_every == 0:\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                \n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/10... Step: 100... Loss: 0.537814... Val Loss: 0.534263\n",
            "Epoch: 1/10... Step: 200... Loss: 0.674150... Val Loss: 0.635464\n",
            "Epoch: 1/10... Step: 300... Loss: 0.537339... Val Loss: 0.614781\n",
            "Epoch: 1/10... Step: 400... Loss: 0.654070... Val Loss: 0.545254\n",
            "Epoch: 1/10... Step: 500... Loss: 0.694650... Val Loss: 0.660270\n",
            "Epoch: 1/10... Step: 600... Loss: 0.359698... Val Loss: 0.454849\n",
            "Epoch: 1/10... Step: 700... Loss: 0.435665... Val Loss: 0.403121\n",
            "Epoch: 1/10... Step: 800... Loss: 0.646020... Val Loss: 0.560039\n",
            "Epoch: 2/10... Step: 900... Loss: 0.484572... Val Loss: 0.372358\n",
            "Epoch: 2/10... Step: 1000... Loss: 0.414708... Val Loss: 0.350274\n",
            "Epoch: 2/10... Step: 1100... Loss: 0.289431... Val Loss: 0.374900\n",
            "Epoch: 2/10... Step: 1200... Loss: 0.340023... Val Loss: 0.344867\n",
            "Epoch: 2/10... Step: 1300... Loss: 0.244881... Val Loss: 0.305493\n",
            "Epoch: 2/10... Step: 1400... Loss: 0.412534... Val Loss: 0.287574\n",
            "Epoch: 2/10... Step: 1500... Loss: 0.262847... Val Loss: 0.283296\n",
            "Epoch: 2/10... Step: 1600... Loss: 0.356934... Val Loss: 0.267036\n",
            "Epoch: 3/10... Step: 1700... Loss: 0.159468... Val Loss: 0.257645\n",
            "Epoch: 3/10... Step: 1800... Loss: 0.105886... Val Loss: 0.264430\n",
            "Epoch: 3/10... Step: 1900... Loss: 0.136748... Val Loss: 0.258276\n",
            "Epoch: 3/10... Step: 2000... Loss: 0.215223... Val Loss: 0.249096\n",
            "Epoch: 3/10... Step: 2100... Loss: 0.246480... Val Loss: 0.241037\n",
            "Epoch: 3/10... Step: 2200... Loss: 0.293415... Val Loss: 0.233602\n",
            "Epoch: 3/10... Step: 2300... Loss: 0.225608... Val Loss: 0.228104\n",
            "Epoch: 3/10... Step: 2400... Loss: 0.104916... Val Loss: 0.222544\n",
            "Epoch: 4/10... Step: 2500... Loss: 0.067003... Val Loss: 0.234871\n",
            "Epoch: 4/10... Step: 2600... Loss: 0.125261... Val Loss: 0.243904\n",
            "Epoch: 4/10... Step: 2700... Loss: 0.140417... Val Loss: 0.247882\n",
            "Epoch: 4/10... Step: 2800... Loss: 0.153171... Val Loss: 0.222736\n",
            "Epoch: 4/10... Step: 2900... Loss: 0.188402... Val Loss: 0.220829\n",
            "Epoch: 4/10... Step: 3000... Loss: 0.148640... Val Loss: 0.213697\n",
            "Epoch: 4/10... Step: 3100... Loss: 0.101207... Val Loss: 0.220927\n",
            "Epoch: 4/10... Step: 3200... Loss: 0.083652... Val Loss: 0.204710\n",
            "Epoch: 5/10... Step: 3300... Loss: 0.023797... Val Loss: 0.212059\n",
            "Epoch: 5/10... Step: 3400... Loss: 0.095328... Val Loss: 0.260360\n",
            "Epoch: 5/10... Step: 3500... Loss: 0.047089... Val Loss: 0.222128\n",
            "Epoch: 5/10... Step: 3600... Loss: 0.140696... Val Loss: 0.224218\n",
            "Epoch: 5/10... Step: 3700... Loss: 0.054017... Val Loss: 0.227821\n",
            "Epoch: 5/10... Step: 3800... Loss: 0.083785... Val Loss: 0.215435\n",
            "Epoch: 5/10... Step: 3900... Loss: 0.164442... Val Loss: 0.223447\n",
            "Epoch: 5/10... Step: 4000... Loss: 0.027329... Val Loss: 0.225902\n",
            "Epoch: 6/10... Step: 4100... Loss: 0.109676... Val Loss: 0.261226\n",
            "Epoch: 6/10... Step: 4200... Loss: 0.014252... Val Loss: 0.272929\n",
            "Epoch: 6/10... Step: 4300... Loss: 0.005513... Val Loss: 0.285717\n",
            "Epoch: 6/10... Step: 4400... Loss: 0.047526... Val Loss: 0.268818\n",
            "Epoch: 6/10... Step: 4500... Loss: 0.011441... Val Loss: 0.259119\n",
            "Epoch: 6/10... Step: 4600... Loss: 0.064618... Val Loss: 0.297297\n",
            "Epoch: 6/10... Step: 4700... Loss: 0.049154... Val Loss: 0.236557\n",
            "Epoch: 6/10... Step: 4800... Loss: 0.004384... Val Loss: 0.270515\n",
            "Epoch: 7/10... Step: 4900... Loss: 0.046363... Val Loss: 0.277478\n",
            "Epoch: 7/10... Step: 5000... Loss: 0.088846... Val Loss: 0.298792\n",
            "Epoch: 7/10... Step: 5100... Loss: 0.286154... Val Loss: 0.273550\n",
            "Epoch: 7/10... Step: 5200... Loss: 0.003855... Val Loss: 0.275842\n",
            "Epoch: 7/10... Step: 5300... Loss: 0.085564... Val Loss: 0.324676\n",
            "Epoch: 7/10... Step: 5400... Loss: 0.060994... Val Loss: 0.265243\n",
            "Epoch: 7/10... Step: 5500... Loss: 0.198186... Val Loss: 0.265788\n",
            "Epoch: 7/10... Step: 5600... Loss: 0.210708... Val Loss: 0.268412\n",
            "Epoch: 8/10... Step: 5700... Loss: 0.009079... Val Loss: 0.310169\n",
            "Epoch: 8/10... Step: 5800... Loss: 0.014387... Val Loss: 0.324020\n",
            "Epoch: 8/10... Step: 5900... Loss: 0.090040... Val Loss: 0.291608\n",
            "Epoch: 8/10... Step: 6000... Loss: 0.144140... Val Loss: 0.293799\n",
            "Epoch: 8/10... Step: 6100... Loss: 0.044924... Val Loss: 0.297158\n",
            "Epoch: 8/10... Step: 6200... Loss: 0.195558... Val Loss: 0.264739\n",
            "Epoch: 8/10... Step: 6300... Loss: 0.015031... Val Loss: 0.270642\n",
            "Epoch: 8/10... Step: 6400... Loss: 0.006944... Val Loss: 0.303747\n",
            "Epoch: 9/10... Step: 6500... Loss: 0.079924... Val Loss: 0.303826\n",
            "Epoch: 9/10... Step: 6600... Loss: 0.133510... Val Loss: 0.345623\n",
            "Epoch: 9/10... Step: 6700... Loss: 0.076555... Val Loss: 0.305672\n",
            "Epoch: 9/10... Step: 6800... Loss: 0.003593... Val Loss: 0.290010\n",
            "Epoch: 9/10... Step: 6900... Loss: 0.008137... Val Loss: 0.299083\n",
            "Epoch: 9/10... Step: 7000... Loss: 0.005025... Val Loss: 0.308229\n",
            "Epoch: 9/10... Step: 7100... Loss: 0.010262... Val Loss: 0.308042\n",
            "Epoch: 9/10... Step: 7200... Loss: 0.000890... Val Loss: 0.303203\n",
            "Epoch: 10/10... Step: 7300... Loss: 0.000731... Val Loss: 0.343685\n",
            "Epoch: 10/10... Step: 7400... Loss: 0.039572... Val Loss: 0.323453\n",
            "Epoch: 10/10... Step: 7500... Loss: 0.003758... Val Loss: 0.332427\n",
            "Epoch: 10/10... Step: 7600... Loss: 0.003904... Val Loss: 0.359687\n",
            "Epoch: 10/10... Step: 7700... Loss: 0.042656... Val Loss: 0.362220\n",
            "Epoch: 10/10... Step: 7800... Loss: 0.008422... Val Loss: 0.323698\n",
            "Epoch: 10/10... Step: 7900... Loss: 0.006887... Val Loss: 0.323440\n",
            "Epoch: 10/10... Step: 8000... Loss: 0.018074... Val Loss: 0.345590\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChpP1njrLc-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ddc7d0-2fd2-4cff-819c-e09a7e4db99e"
      },
      "source": [
        "\n",
        "test_losses = [] \n",
        "num_correct = 0\n",
        "\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "   \n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    pred = torch.round(output.squeeze())  \n",
        "    \n",
        "    \n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.008\n",
            "Test accuracy: 0.998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9lCQK_6LzUj"
      },
      "source": [
        "def preprocess(review):\n",
        "    review = review.lower()\n",
        "    word_list = review.split()\n",
        "    num_list = []\n",
        "   \n",
        "    reviews_int = []\n",
        "    for word in word_list:\n",
        "        if word in review_dict.token2id:\n",
        "            num_list.append(review_dict.token2id[word])\n",
        "    reviews_int.append(num_list)\n",
        "    return reviews_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DacOT_EzwVFf"
      },
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "  \n",
        "    int_rev = preprocess(test_review)\n",
        "    \n",
        "    features = pad_features(int_rev, L=seq_length)\n",
        "    \n",
        "    features = torch.from_numpy(features)\n",
        "    \n",
        "    net.eval()\n",
        "    val_h = net.init_hidden(1)\n",
        "    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        features = features.cuda()\n",
        "\n",
        "    output, val_h = net(features, val_h)\n",
        "    \n",
        "    pred = torch.round(output)\n",
        "    \n",
        "    sent = [\"Positive\" if pred.item() == 1 else \"Negative\"]\n",
        "    \n",
        "    print(sent,'  ',((pred.item()==1)*output.item()+(pred.item()==0)*(1-output.item()))*100,'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKTNtmG52Rh"
      },
      "source": [
        "positive_review = 'A great movie. Totally worth every penny. '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAIVDzb7wp9J",
        "outputId": "297a4d73-530e-49c7-a37d-4ace3d9e08e6"
      },
      "source": [
        "seq_length=200\n",
        "predict(net, positive_review, seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Positive']    98.46177101135254 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAsPbHIYAJs9"
      },
      "source": [
        "negative_review = 'Amongst the worst works of diresctor nick furry. I want my money back. Waste of time.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-_tR_sYAyBt",
        "outputId": "624bee15-f929-4974-fe65-462405130534"
      },
      "source": [
        "seq_length=200\n",
        "predict(net, negative_review, seq_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Negative']    99.9260982265696 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz-u69ak_zOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27de3c00-483e-4bba-d670-bf99f2f90ac1"
      },
      "source": [
        "\n",
        "\n",
        "test_losses = [] \n",
        "num_correct = 0\n",
        "\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "for inputs, labels in valid_loader:\n",
        "\n",
        " \n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    \n",
        "    pred = torch.round(output.squeeze())  \n",
        "    \n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "\n",
        "test_acc = num_correct/len(valid_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.345\n",
            "Test accuracy: 0.933\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}