{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzhW8jOeUhCZ1LRxgxQ6/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreedharmalpani/soc_check/blob/main/GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW8ubJ8izbGc"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDzTcmDOzfPo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5570J5ozfab"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/IMDB Dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pogPW9x6zf3o"
      },
      "source": [
        "def splitDF(r):\n",
        "  dataLen = len(df)\n",
        "  temp = ['test']*int((1-r)*dataLen) + ['train']*int((r)*dataLen)\n",
        "  random.shuffle(temp)\n",
        "  df['split'] = temp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsgJkYf-zhCO"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-PgbBDvzhLl"
      },
      "source": [
        "df['review'] = df['review'].apply(lambda x:x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3Wz7q0LzhTB"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83xOVN7xzhar"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\", \"!\", '\"', ',','#','$','@','%','^','&','*'))\n",
        "    return final\n",
        "\n",
        "df['review'] = df['review'].apply(remove_punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQLwvIkazxSt"
      },
      "source": [
        "def remove_tags(text):\n",
        "    final = \"\"\n",
        "    stt = True\n",
        "    for char in text:\n",
        "        if char == '<':\n",
        "            stt = False\n",
        "        if(stt):\n",
        "            final = final + char\n",
        "        if char == '>':\n",
        "            stt = True\n",
        "            final = final + ' '\n",
        "    return final\n",
        "    \n",
        "df['review'] = df['review'].apply(remove_tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj4v8Jl6zxPR"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F73UAVegzxMl"
      },
      "source": [
        "df['review'] = df['review'].apply(nltk.word_tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjcBW1qSzxIq"
      },
      "source": [
        "def stem_tokens(tokens):\n",
        "    final = [nltk.stem.PorterStemmer().stem(word) for word in tokens]\n",
        "    return final\n",
        "    \n",
        "df['review'] = df['review'].apply(stem_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6xYMcbazxGG"
      },
      "source": [
        "df['len_review'] = df['review'].apply(lambda x:len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcjDG44izxDK"
      },
      "source": [
        "df['sentiment'] = [1*(sent=='positive') for sent in df['sentiment']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzpVnZoQzxAs"
      },
      "source": [
        "from gensim import corpora\n",
        "\n",
        "review_dict = corpora.Dictionary(df['review'])\n",
        "VOCAB_SIZE = len(review_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DegVsdOQzw9z"
      },
      "source": [
        "review_int = []\n",
        "for review in df['review']:\n",
        "  temp = [review_dict.token2id[x] for x in review]\n",
        "  review_int.append(temp)\n",
        "df['review_int'] = review_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZwntcKfzw7d"
      },
      "source": [
        "review_int = df['review_int']\n",
        "review_len = df['len_review']\n",
        "review_label = df['sentiment']\n",
        "review_int = [review_int[i] for i,x in enumerate(review_int) if review_len[i]>0]\n",
        "review_label = [review_label[i] for i,x in enumerate(review_label) if review_len[i]>0]\n",
        "review_len = [review_len[i] for i,x in enumerate(review_len) if review_len[i]>0 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iabIU9Z9zw4f"
      },
      "source": [
        "def pad_features(review_int, L):\n",
        "  features = np.zeros((len(review_int),L), dtype = int)\n",
        "  for i,review in enumerate(review_int):\n",
        "    l = len(review)\n",
        "    if l <= L:\n",
        "      patch = [0]*(L-l)\n",
        "      new = patch + review\n",
        "      features[i,:] = np.array(new)\n",
        "    else :\n",
        "      features[i,:] = np.array(review[:L])\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM-njZOZzvfm"
      },
      "source": [
        "features = pad_features(review_int, 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eh5ZEnVzhh-"
      },
      "source": [
        "split_frac = 0.7\n",
        "\n",
        "train_x = features[0:int(len(features)*split_frac)]\n",
        "train_y = np.array(review_label[0:int(len(features)*split_frac)])\n",
        "\n",
        "test_x = features[len(features)-int(len(features)*split_frac):len(features)-int(len(features)*split_frac/2)]\n",
        "test_y = np.array(review_label[len(features)-int(len(features)*split_frac):len(features)-int(len(features)*split_frac/2)])\n",
        "\n",
        "valid_x = features[len(features)-int(len(features)*split_frac/2):]\n",
        "valid_y = np.array(review_label[len(features)-int(len(features)*split_frac/2):])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHpXf1wbzhor"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifNxFzeYzhvC"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwovaHA-0YhR"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxFCRnIl0YcS"
      },
      "source": [
        "class SentimentGRU(nn.Module):\n",
        "    \"\"\"\n",
        "    The GRU model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and GRU layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "  \n",
        "        # embeddings and gru_out\n",
        "        embeds = self.embedding(x)\n",
        "        gru_out, hidden = self.gru(embeds, hidden)\n",
        "    \n",
        "        # stack up gru outputs\n",
        "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(gru_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLLcpG-f0YZ2"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(review_dict)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentGRU(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWmuDl5T0YXJ"
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW7ndBio0YUR"
      },
      "source": [
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRXSMWGV0YRa"
      },
      "source": [
        "# training params\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        #print(counter)\n",
        "       \n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = h.data\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in GRUs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = h.data\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaFDvEjX0YPC"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = h.data\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpLlUl_L0YMS"
      },
      "source": [
        "def preprocess(review):\n",
        "    review = review.lower()\n",
        "    word_list = review.split()\n",
        "    num_list = []\n",
        "    #list of reviews\n",
        "    #though it contains only one review as of now\n",
        "    reviews_int = []\n",
        "    for word in word_list:\n",
        "        if word in review_dict.token2id:\n",
        "            num_list.append(review_dict.token2id[word])\n",
        "    reviews_int.append(num_list)\n",
        "    return reviews_int"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA4v4Pgd0sbZ"
      },
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "    ''' Prints out whether a give review is predicted to be \n",
        "        positive or negative in sentiment, using a trained model.\n",
        "        \n",
        "        params:\n",
        "        net - A trained net \n",
        "        test_review - a review made of normal text and punctuation\n",
        "        sequence_length - the padded length of a review\n",
        "        '''\n",
        "    #change the reviews to sequence of integers\n",
        "    int_rev = preprocess(test_review)\n",
        "    #pad the reviews as per the sequence length of the feature\n",
        "    features = pad_features(int_rev, L=seq_length)\n",
        "    \n",
        "    #changing the features to PyTorch tensor\n",
        "    features = torch.from_numpy(features)\n",
        "    \n",
        "    #pass the features to the model to get prediction\n",
        "    net.eval()\n",
        "    val_h = net.init_hidden(1)\n",
        "    val_h = val_h.data\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        features = features.cuda()\n",
        "\n",
        "    output, val_h = net(features, val_h)\n",
        "    \n",
        "    #rounding the output to nearest 0 or 1\n",
        "    pred = torch.round(output)\n",
        "    \n",
        "    #mapping the numeric values to postive or negative\n",
        "    sent = [\"Positive\" if pred.item() == 1 else \"Negative\"]\n",
        "    \n",
        "    # print custom response based on whether test_review is pos/neg\n",
        "    print(sent,'  ',((pred.item()==1)*output.item()+(pred.item()==0)*(1-output.item()))*100,'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzBWj1510YJq"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in valid_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = h.data\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(valid_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC9IQUCm0YGu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Td6INgp0YD_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZWoKAec0YBL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAJHqgyq0X-g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FLwvVImzh1K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqzJJGCLzh7R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCZAEBlyziBK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}